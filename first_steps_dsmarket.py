# -*- coding: utf-8 -*-
"""Copia_de_first_steps_dsmarket_25032024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vz0qh_T2kVz2pAmDDYtSgYdAnuJrsjnI

# DSMarket - First look at input files

## 1. Importing libraries
"""

import numpy as np
import pandas as pd

pd.options.display.float_format = '{:,.2f}'.format
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)

"""## 2. Paths and directories"""

#sales_data_path = "/data_dsmarket/item_sales.csv"
#calendar_data_path = "../data_dsmarket/daily_calendar_with_events.csv"
#prices_data_path = "../data_dsmarket/item_prices.csv"

"""## 3. Import files

### 3A. Sales data
"""

from google.colab import drive
drive.mount('/content/drive')

pd_sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSMarket/data_dsmarket/item_sales.csv')

pd_sales.head()

pd_sales.region.value_counts()

pd_sales.info(verbose=False)

"""How many **stores** are there in the data?"""

print("There are {} stores in the data, and the number of registers per store are: ".format(pd_sales.store.nunique()))
pd_sales.store.value_counts()

"""How many **items**?"""

print("There are {} items in the data, and the number of registers per  are: ".format(pd_sales.item.nunique()))
pd_sales.item.value_counts()

"""And **departments**?"""

print("There are {} departments in the data, and the number of registers per department are: ".format(pd_sales.department.nunique()))
pd_sales.department.value_counts()

"""Do we have one register per id? In other words, can two registers have the same id?"""

print(pd_sales.shape)
print(pd_sales.id.drop_duplicates().shape)

pd_sales[pd_sales.duplicated()]

pd_sales['d_1'].dtype

pd_sales['sales_total'] = pd_sales.loc[:,'d_1':].fillna(0).sum(axis=0)

#creo columna con la suma de las ventas

columnas_rango = pd_sales.columns[pd_sales.columns.get_loc('d_1'):pd_sales.columns.get_loc('d_1913')+1]

pd_sales['sales_total'] = pd_sales[columnas_rango].sum(axis=1)

pd_sales

#elimino las columnas por dias

columns_to_drop = pd_sales.columns[7:-1]  # Select columns from index 7 to -3 (inclusive)
pd_sales.drop(columns_to_drop, axis=1, inplace=True)

pd_sales.head()

pd_sales[pd_sales.id == 'ACCESORIES_1_002_NYC_1'].T

pd_sales.info()

"""### 3B. Calendar data"""

pd_calendar = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSMarket/data_dsmarket/daily_calendar_with_events.csv')
print("The shape of the calendar datataframe is :", pd_calendar.shape)

pd_calendar.head()

pd_calendar.info(verbose=True)

print("The range of dates available are: {} - {}".format(pd_calendar['date'].min(),pd_calendar['date'].max()))

"""What sort of events do we have?"""

pd_calendar.event.value_counts()

"""We don't have a lot of events, but we can think about including additional ones in a later stage (it might help!)

### 3C. Prices data
"""

pd_prices = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DSMarket/data_dsmarket/item_prices.csv')
print("The shape of the prices datataframe is :", pd_prices.shape)

pd_prices.head()

pd_prices.info(verbose=True)

"""Same number of items in the prices data?"""

print("There are {} items in the data, and the number of registers per item are: ".format(pd_prices.item.nunique()))
pd_prices.item.value_counts()

"""The number of items does match, but there are many registers per item. It seems that prices per item can change with time

Let's take a loook at the variables distribution of the prices dataframe
"""

pd_prices.describe()

pd_prices.describe(include=['object'])

pd_prices.head()

pd_prices.isnull().sum()

pd_prices = pd_prices.dropna()

pd_prices['sell_price'].nunique()

pd_prices.columns

pd_prices.info()

pd_prices['id'] = pd_prices['item'] + '_' + pd_prices['store_code']

pd_prices.info()

pd_prices['yearweek'].value_counts()

# Only convert if the data type isn't already string
if pd_prices['yearweek'].dtype != 'object' and pd_prices['yearweek'].dtype != 'str':
    pd_prices['yearweek'] = pd_prices['yearweek'].astype(str)

pd_prices["yearweek"] = pd_prices["yearweek"].str[:6].astype(int)

pd_prices['yearweek'] = pd_prices['yearweek'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:])

from datetime import datetime, timedelta
# Función para extraer la fecha de un yearweek
def extraer_fecha(yearweek):
    año, semana = map(int, yearweek.split('-'))
    fecha = datetime.strptime(f'{año}-W{semana}-1', '%Y-W%W-%w')
    return fecha

# Aplicar la función extraer_fecha a la columna 'yearweek'
pd_prices['date'] = pd_prices['yearweek'].apply(extraer_fecha)

# Mostrar el DataFrame resultante
pd_prices.head()

pd_prices.info()

from datetime import datetime, timedelta
def weekyearnum(dt):
    return dt.strftime("%Y%W")

def myweeyearknum(dt):
    offsetdt = dt + timedelta(days=+2);  # you add 3 days to Mon to get to Thu
    return weekyearnum(offsetdt)

def weeknum(dt):
    return dt.isocalendar()[1]

def myweeknum(dt):
    offsetdt = dt + timedelta(days=+2);  # you add 3 days to Mon to get to Thu
    return weeknum(offsetdt)

## Merge
pd_calendar['date'] = pd.to_datetime(pd_calendar['date'], format = "%Y-%m-%d")

pd_calendar['yearweek'] = pd_calendar['date'].apply(lambda x: myweeyearknum(x))

pd_calendar

pd_sales.info()

pd_prices.info()

pd_prices.head()

pd_sales.describe(include=['object'])

pd_prices.describe(include=['object'])

# Filtrar 'pd_prices' por valores únicos en 'id'
pd_prices_filtered = pd_prices.drop_duplicates(subset=['id'])

# Filtrar 'pd_prices' por valores únicos en 'id'
pd_prices_filtered = (
    pd_prices.groupby('id').head(1)
)

pd_prices_filtered.describe(include=['object'])

pd_prices_filtered.isnull().sum()

pd_sales.isnull().sum()

pd_sales.head()

pd_prices_filtered['id'].dtype

pd_sales['id'].dtype

pd_sales.index.dtype

pd_prices_filtered.index.dtype

print(pd_prices_filtered.shape)
print(pd_sales.shape)
pd_merged2 = pd.merge(pd_sales,pd_prices_filtered, how="left",
                   left_on = ['id', 'item', 'store_code', 'category'], right_on = ['id', 'item', 'store_code', 'category'])
print(pd_merged2.shape)

pd_merged2.head(10)

pd_merged2.info()

pd_merged2['date'].value_counts()

pd_merged2['region'].value_counts()

pd_merged2['store'].value_counts()

pd_merged2['sales_total'].value_counts()

pd_merged2['sell_price'].value_counts()

pd_merged2['yearweek'].value_counts()

pd_merged2['Revenue'] = pd_merged2['sell_price'] * pd_merged2['sales_total']

df=pd_merged2

df.head(10)

df[df.duplicated()]

import csv

# Guardar el DataFrame como un archivo CSV
df.to_csv('df.csv')

df.info()

pd.to_pickle(df, "/content/drive/MyDrive/Colab Notebooks/DSMarket/df_final_PK")

df = pd.read_pickle("/content/drive/MyDrive/Colab Notebooks/DSMarket/df_final_PK")

df.head()

