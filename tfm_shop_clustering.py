# -*- coding: utf-8 -*-
"""TFM_SHOP_CLUSTERING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vL0sHA_EUc0ZpLuTijsw5GZc-gkrOWK
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

pd.options.display.float_format = '{:,.2f}'.format
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)

df = pd.read_pickle("/content/drive/MyDrive/Colab Notebooks/DSMarket/df_final_PK")

df.head()

df.info()

df['store'].value_counts()

##posibles nuevas variables:
'''
-max_venta
-min_venta
-mean_venta
-fecha ultima compra
-mes_compra
-compra por semanas
-agrupando por ventas podemos sacar por ejem, los 15 items mas populares:
most_important_item = df.groupby(['item'])['sales_total'].size().sort_values().tail(15).index.tolist()
df['top_items']=df['item'].isin(most_important_item)
- agrupar los items por sell_price por fecha para ver como cambia a lo largo tiempo
##sell_price_evolution= df.groupby(['item', 'date'])['sell_price'].size().sort_values().tail(15).index.tolist()
df["sell_price_evolution"] = df.groupby(['item', 'date'])['sell_price'].transform(np.mean)
df["sell_price_evolution"] = df.groupby(['item', 'date'])['sales_total'].transform(np.sum)

-agrupando podemos sacar el weekday que mas se vende un item
'''

most_important_store = df.groupby(['store'])['sales_total'].size().sort_values().tail(3).index.tolist()

df['top_store']=df['store'].isin(most_important_store)

df['top_store'].value_counts()

less_important_store = df.groupby(['store'])['sales_total'].size().sort_values().head(3).index.tolist()
df['less_store']=df['store'].isin(less_important_store)

df['less_store'].value_counts()

df["sales_store_category_sum"] = df.groupby(['store', 'category'])['sales_total'].transform(np.sum)

df["revenue_store_category_mean"] = df.groupby(['store', 'category'])['Revenue'].transform(np.mean)

df["sales_store_region_sum"] = df.groupby(['store', 'region'])['sales_total'].transform(np.sum)

df["revenue_store_region_mean"] = df.groupby(['store', 'region'])['Revenue'].transform(np.mean)

df["sell_price_evolution_mean"] = df.groupby(['store', 'date'])['sell_price'].transform(np.mean)

df['sell_price_evolution_mean']. nunique()

df['sell_price'].nunique()

df.tail(3)

aggregated_Revenue = df.groupby('store').agg(
    max_Revenue = ('Revenue', 'max'),
    min_Revenue = ('Revenue', 'min'),
    mean_Revenue = ('Revenue', 'mean')

)

df = pd.merge(df, aggregated_Revenue, on = 'store')

df.head(3)

df['max_Revenue'].nunique()

df.info()

df["month_sale"] = df["date"].dt.month
df["year_sale"] = df["date"].dt.year

df["sales_total_evolution"] = df.groupby(['store', 'date'])['sales_total'].transform(np.sum)

df['last_sale_date'] = df['date'].max()

df['time_since_last_sale'] =\
df['last_sale_date'] - df['date']

df["time_since_last_sale"] = df["time_since_last_sale"].dt.days

df.head(3)

# Commented out IPython magic to ensure Python compatibility.
# silence warnings
import warnings
warnings.filterwarnings("ignore")

# operating system
import os

# time calculation to track some processes
import time

# numeric and matrix operations
import numpy as np
import pandas as pd

# loading ploting libraries
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# python core library for machine learning and data science
import sklearn
from sklearn import set_config
set_config(transform_output = "pandas")

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import RobustScaler, MinMaxScaler
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.cluster import KMeans

print("Working with this sklearn version {}".format(sklearn.__version__))

df.info()

df.columns.tolist()

df.set_index("store", inplace = True)
lc=[
 'sell_price',
 'Revenue',
 'top_store',
 'less_store',
 'sales_store_category_sum',
 'revenue_store_category_mean',
 'sales_store_region_sum',
 'revenue_store_region_mean',
 'sell_price_evolution_mean',
 'max_Revenue',
 'min_Revenue',
 'mean_Revenue',
 'month_sale',
 'year_sale',
 'sales_total_evolution',
 'sales_total',
  'time_since_last_sale']

df = df[lc]

def build_unique_id_features(X):
    aggregated_df = X.groupby(X.index).agg(
        amount_sales=('Revenue', 'sum'),
        avg_ticket=('Revenue', 'mean'),
        avg_sales =('sales_total_evolution','mean'),
        avg_sell_price = ('sell_price_evolution_mean', 'mean'),


        last_sale = ('time_since_last_sale', 'min'),
        first_sale = ('time_since_last_sale', 'max'),
        month_sales_max =('month_sale', 'mean'),
        year_sales_max = ('year_sale', 'mean'),


        most_popular_store=('top_store', 'sum'),
        less_popular_store=('less_store', 'sum'),
        category_sales =('sales_store_category_sum', 'sum'),
        category_revenue = ('revenue_store_category_mean', 'sum')

    )
    return aggregated_df

## mean_sales_per_store=X[['store_code_BOS_1', 'store_code_BOS_2', 'store_code_BOS_3', 'store_code_NYC_1', 'store_code_NYC_2', 'store_code_NYC_3', 'store_code_NYC_4', 'store_code_PHI_1', 'store_code_PHI_2', 'store_code_PHI_3']].mean(),
    ##    max_seller_store=X[['store_code_BOS_1', 'store_code_BOS_2', 'store_code_BOS_3', 'store_code_NYC_1', 'store_code_NYC_2', 'store_code_NYC_3', 'store_code_NYC_4', 'store_code_PHI_1', 'store_code_PHI_2', 'store_code_PHI_3']].max()

ProductIdFeatureGenerator = FunctionTransformer(func = build_unique_id_features)

# separamos el pipeline del a loop, para no tener que volver a hacer los primeros 3 pasos para cada k de la loop
pipe = Pipeline(steps = [
    ("Imputer", KNNImputer()),
    ("CustomTransformer", ProductIdFeatureGenerator),
    ("RobustScaler", RobustScaler(quantile_range = (0, 99.0)))
])

df_scaled_transformed = pipe.fit_transform(df)

sse = {}

for k in range(2, 10):

    print(f"Fitting pipe with {k} clusters")

    clustering_model = KMeans(n_clusters = k)
    clustering_model.fit(df_scaled_transformed)

    sse[k] = clustering_model.inertia_

#elbow curve

fig = plt.figure(figsize = (16, 8))
ax = fig.add_subplot()

x_values = list(sse.keys())
y_values = list(sse.values())

ax.plot(x_values, y_values, label = "Inertia/dispersión de los clústers")
fig.suptitle("Variación de la dispersión de los clústers en función de la k", fontsize = 16);

pipe = Pipeline(steps = [
    ("Imputer", KNNImputer()),
    ("CustomTransformer", ProductIdFeatureGenerator),
    ("RobustScaler", RobustScaler(quantile_range = (0, 99.0))),
    ("Clustering", KMeans(n_clusters = 4, random_state = 175))
])

df.shape

pipe.fit(df)

X_processed = pipe[:2].transform(df)

labels = pipe.predict(df)

# le asignamos al DataFrame procesado el clúster.
# si lo hacemos al df escalado será más díficil de interpretar los resultados porque los números están escalados
X_processed["cluster"] = labels

X_processed.shape

"""FICHA

"""

ficha_df = pd.DataFrame()

resumen_data_list = []
for i, col in enumerate(["amount_sales", "avg_sell_price","last_sale", "most_popular_store", "less_popular_store"]):
    resumen_data = X_processed[["cluster", col]].groupby("cluster").describe().T[1:]
    resumen_data_list.append(resumen_data)

ficha_df = pd.concat(resumen_data_list, ignore_index=True)

# generamos nuestro multiindex

out_index = [
    "Monetarios",
    "Evolucion precio",
    "Frecuencia",
    "Mas popularidad",
    "Menos popularidad"

]

inner_index = [
    "Importe",
    "precios",
    "última venta",
    "top ventas",
    "low ventas"


]

estadisticos = ["Media", "Desviación", "Mínimo", "Perc. 25", "Perc. 50", "Perc. 75", "Máximo"]

new_multi_index = []

for oi, ii, in zip(out_index, inner_index):
    for es in estadisticos:
        new_multi_index.append((oi, ii, es))

def generate_multiindex(list_of_tuples, names):
    return pd.MultiIndex.from_tuples(list_of_tuples, names = names)

names = ["Grupo Indicadores", "Indicador", "Estadístico"]
index_ficha = generate_multiindex(new_multi_index, names)
ficha_df.set_index(index_ficha, inplace = True)

ficha_df = ficha_df.rename(columns = {
    0 : "Tiendas premium",
    1 : "De menor stock",
    2 : "Tiendas mas rentables",
    3 : "Tiendas 'marca blanca'",
    4 : "Tiendas menos "
})

ficha_df.style.background_gradient(cmap = 'Blues', axis = 1)

# El primer cluster es el que aglutina las tiendas con unos precios más altos y que más han evolucionado(tienda premium),
# El segundo cluster aglutina las tiendas con un menor tiempo de stock(mayor flujo) de los productos,
# El tercer cluster aglutina las tiendas con  mayores ingresos por ventas(solo 1 tienda) y es la mas popular, es decir la que mas vende
# El cuarto cluster aglutina las tiendas con los productos menos populares ventas, las que menos

(
    X_processed
    .groupby("cluster")
    .describe()
    .T
    .style.background_gradient(cmap = 'Blues', axis = 1)
)

X_processed

