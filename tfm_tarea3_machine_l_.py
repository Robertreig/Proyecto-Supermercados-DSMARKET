# -*- coding: utf-8 -*-
"""TFM_TAREA3_MACHINE_L..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LXe-_5qjSakCQBcFKgOtNST9e4Gyer5g
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files

from scipy import stats

from datetime import datetime
from sklearn import model_selection
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier

from google.colab import drive

df = pd.read_pickle("/content/drive/MyDrive/Colab Notebooks/DSMarket/df_final_PK")

"""#DATA UNDERSTANDING"""

df.info(verbose=False)

df.head()

df.describe(include =np.number).T

df.describe(exclude= np.number).T

del(df["id"])

df[df.duplicated()]

df.isnull().sum()

df['yearweek'] = df['yearweek'].apply(lambda x: x.replace('-', ''))

df['yearweek'].value_counts()

df['yearweek'] = df['yearweek'].astype(int)

df.info()

"""###Analisi del Target

"""

TARGET ='sales_total'

df[TARGET].describe()

df[TARGET].isnull().sum()

plt.figure(figsize=(15,5))
sns.displot(df[TARGET])

##aplicar transformacion al TARGET, logaritmica, para normalizar el TARGET
#1. Traer los outliers para dentro
#2. sera mas facil comparar con otras variables

df['sales_totalLOG'] = df[TARGET].apply(lambda x: np.log1p(x))

TARGET_LOG = 'sales_totalLOG'

plt.figure(figsize=(15,5))
sns.displot(df[TARGET_LOG])

def plot_cat_values(dataframe, column, target_col):
  plt.figure(figsize=(15,5))
  ax1=plt.subplot(2,1,1)

  ax1=sns.countplot(data= dataframe,
                    x = column,
                    order=list(dataframe[column].unique()))

  ax2=plt.subplot(2,1,2)
  ax2=sns.boxplot(data= dataframe[dataframe[target_col]>0],
              x = column,
              y= target_col,
              order= list(dataframe[column].unique()))
  plt.show()

plot_cat_values(df, 'store', TARGET_LOG)

df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month

"""Nuevas variables"""

most_important_item = df.groupby(['item'])['sales_total'].size().sort_values().tail(15).index.tolist()

df['top_items']=df['item'].isin(most_important_item)

df['top_items'].value_counts()

less_important_items = df.groupby(['item'])['sales_total'].size().sort_values().head(15).index.tolist()
df['less_items']=df['item'].isin(less_important_items)

df['less_items'].value_counts()

df["sell_price_evolution"] = df.groupby(['item', 'date'])['sell_price'].transform(np.mean)

df['last_purchase_date'] = df['date'].max()
df['last_purchase_date'] = df['last_purchase_date'].astype(np.int64)

df['date']= df['date'].astype(np.int64)

df['time_since_last_purchase'] =\
df['last_purchase_date'] - df['date']

most_important_store = df.groupby(['store'])['sales_total'].size().sort_values().tail(3).index.tolist()

df['top_store']=df['store'].isin(most_important_store)

df['top_store'].value_counts()

less_important_store = df.groupby(['store'])['sales_total'].size().sort_values().head(3).index.tolist()
df['less_store']=df['store'].isin(less_important_store)

df.set_index('item', inplace = True)

df.drop('date', axis = 1, inplace = True)

corr=df.corr(numeric_only=True)

corr.style.background_gradient(cmap="coolwarm")

#eliminamos las variables 'yearweek' y 'sell_price' por estar muy correlacionadas con . Tb se elimina last_purchase_date y time_since_last_purchase

df.drop(['yearweek', 'sell_price', 'last_purchase_date', 'time_since_last_purchase'], axis = 1, inplace = True)

"""## Data Preparation

"""

def OHE(dataframe, column_name):
    _dummy_dataset = pd.get_dummies(dataframe[column_name], prefix=column_name)
    dataframe = pd.concat([dataframe, _dummy_dataset], axis=1)
    return dataframe.drop(column_name, axis=1)

ohe_list=['category', 'department', 'store', 'store_code', 'region']

for column in ohe_list:
  df = OHE(df, column)

df.info()

"""## Modelling

"""

target_linked_features = ['Revenue', 'sales_total']

df.drop(target_linked_features, axis = 1 , inplace = True)

variables_numericas = df.select_dtypes(include='number')

df.pivot_table(index=['year','month'], values= TARGET_LOG, aggfunc = [len, np.mean])

"""\#nos quedamos desde enero de 2013 puesto que las ventas han ido decreciendo en los últimos años"""

df_val = df[df['year']*100 + df['month'] >= 201300]
df_dev = df[df['year']*100 + df['month'] < 201300]

"""## Fiting y evaluacion

"""

df_dev_X = df_dev.drop(TARGET_LOG, axis = 1)
df_val_X = df_val.drop(TARGET_LOG, axis = 1)

df_dev_y = df_dev[[TARGET_LOG]]
df_val_y = df_val[[TARGET_LOG]]

df_dev_X.shape

model_selection.train_test_split??

X_train_1, X_test_1, y_train_1, y_test_1 = model_selection.train_test_split(df_dev_X, df_dev_y, test_size=0.3, random_state=42)

model1_rf = RandomForestRegressor(n_estimators= 50, max_depth = 3, random_state = 42)

model1_rf.fit(X_train_1,y_train_1)

from sklearn.tree import export_graphviz

tree1 = export_graphviz(
    decision_tree = model1_rf[1],
    feature_names = X_train_1.columns
)

import graphviz
graphviz.Source(tree1)



#feature importance

top_features = pd.Series(model1_rf.feature_importances_, index = X_train_1.columns).sort_values(ascending = False).head(15)

top_features

predictions_rf1 = model1_rf.predict(X_test_1)

test_predictions_rf1 = pd.DataFrame(predictions_rf1, columns = ['Prediction-RF'], index = X_test_1.index)

results_df_rf1 = y_test_1.copy()

results_df_rf1.columns = ['Target']

results_df_rf1 = results_df_rf1.join(test_predictions_rf1)

results_df_rf1.head(10)

results_df_rf1.sort_values(by= 'Target', ascending = False).head(20)

# Las predicciones son un poco bajas necesita reajustes

results_df_rf1['error-RF'] = results_df_rf1['Target'] - results_df_rf1['Prediction-RF']

results_df_rf1['squared_error-RF'] = results_df_rf1['error-RF']**2

results_df_rf1['root_squared_error-RF'] = np.sqrt(results_df_rf1['squared_error-RF'])

results_df_rf1['squared_error-RF'].mean()

from sklearn import metrics

metrics.mean_squared_error(y_test_1,predictions_rf1)

#referencias:

#benchmark: modelo tonto _>

(results_df_rf1['Target']**2).mean()

results_df_rf1.sort_values(by= 'Target', ascending = False).head(5)

#distribucion del error

sns.histplot(data = results_df_rf1[results_df_rf1['Target']>0], x= 'root_squared_error-RF')

#distribucion del target
sns.histplot(data = results_df_rf1[results_df_rf1['Target']>0], x= 'Target')

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

models_to_train = [

    ('Random Forest depth3', RandomForestRegressor(n_estimators = 70, max_depth = 6, random_state = 42)),
    ('DecissionTree', DecisionTreeRegressor(max_depth = 6, random_state = 42)),
    ('GradienBoosting-n_estimators80', xgb.XGBRegressor(n_estimators = 80,max_depth = 4, random_state = 42)),
    ('Linear Regression', LinearRegression())]

def AutoModelling(dev_Dataframe, val_Dataframe, target_var,test_size_prop, models):

  dev_Dataframe_X = dev_Dataframe.drop(target_var, axis = 1)
  val_Dataframe_X = val_Dataframe.drop(target_var, axis = 1)
  dev_Dataframe_y = dev_Dataframe[[target_var]]
  val_Dataframe_y = val_Dataframe[[target_var]]

  X_train, X_test, y_train, y_test = model_selection.train_test_split(dev_Dataframe_X, dev_Dataframe_y, test_size=test_size_prop, random_state=42)
  results_df = y_test.copy()
  results_df.columns = ['Target']

  for model in models:

    model_name = model[0]
    model_instance = model[1]
    model_instance.fit(X_train,y_train)
    predictions = model_instance.predict(X_test)
    test_predictions = pd.DataFrame(predictions, columns = [str('Prediction-'+model_name)], index = X_test.index)
    results_df = results_df.join(test_predictions)

    results_df[str('error-'+model_name)] = results_df['Target'] - results_df[str('Prediction-'+model_name)]
    results_df[str('squared_error-'+model_name)] = results_df[str('error-'+model_name)]**2
    results_df[str('root_squared_error-'+model_name)] = np.sqrt(results_df[str('squared_error-'+model_name)])

    #train
    mse_train = metrics.mean_squared_error(y_train,model_instance.predict(X_train))
    #test
    mse_test = metrics.mean_squared_error(y_test,predictions)
    #val
    mse_val = metrics.mean_squared_error(val_Dataframe_y,model_instance.predict(val_Dataframe_X))
    print('MSE for {}: train {}, test {}, val {}'.format(model_name, np.round(mse_train,4), np.round(mse_test,4), np.round(mse_val,4)))

  return results_df

resultados_modelos = AutoModelling(df_dev, df_val, TARGET_LOG,0.3, models_to_train)

# Es un dataset con pocos registros (30490) y al hacer las particiones en validacion se quedan apenas 90000 resgitros lo que es un poco justo para hacer
# predicciones y por eso es posible que haga que el error en las predicciones sea bastante alto.
# También destacar que el dataset está desbalanceado  ya que el numero de registros con el tiempo ha decrecido de forma drástica.

"""Rebalanceo"""

#validacion no se toca

#cambiamos dev

df_dev_menor8 = df_dev[df_dev[TARGET_LOG]<8]
df_dev_mayor8 = df_dev[df_dev[TARGET_LOG]>=8]

df_dev_menor8.shape

df_dev_mayor8.shape

n_mayor8 = len(df_dev_mayor8)*3

df_dev_menor8_sample = df_dev_menor8.sample(n= n_mayor8, random_state=42)

df_dev_sample = pd.concat([df_dev_mayor8,df_dev_menor8_sample])

models_to_train_sample = [
    ('Random Forest depth3', RandomForestRegressor(n_estimators = 70, max_depth = 5, random_state = 42)),
    ('DecissionTree', DecisionTreeRegressor(max_depth = 5, random_state = 42)),
    ('GradienBoosting-n_estimators80', xgb.XGBRegressor(n_estimators = 80,max_depth = 5, random_state = 42)),
    ('Linear Regression', LinearRegression())]

resultados_modelos = AutoModelling(df_dev_sample, df_val, TARGET_LOG,0.3, models_to_train_sample)

resultados_modelos.sort_values(by = 'Target', ascending = False).head(10)

# Aunque no mejora las metricas, vemos que ahora predice valores un poco mas altos